<h1 align="center">ğŸ¤– Assistive Arm for Cooking ğŸ³</h1>

This project simulates an assistive robotic arm capable of performing **stirring**, **flipping**, and **pouring** actions based on **hand gestures** recognized via webcam. Using **MediaPipe** for gesture detection and **PyBullet** for real-time physics simulation, the robotic arm mimics actions useful in a kitchen setting.

## ğŸ“ Directory Structure
Assistive-Arm-For-Cooking/ <br/>
â”œâ”€â”€ gesture_arm_control.py<br/>
â”œâ”€â”€ instructions.txt <br/>
â”œâ”€â”€ model.urdf <br/>
â”œâ”€â”€ README.md <br/>
â””â”€â”€ requirements.txt <br/>


## ğŸ” Features

- **Hand Gesture Recognition** using [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands)
- **Simulated Robot Arm Control** using [PyBullet](https://pybullet.org/)
- **Three Cooking Motions**:
  - ğŸŒ€ **Stir**
  - ğŸ”„ **Flip**
  - ğŸ’§ **Pour**
- Real-time gesture tracking via webcam
- Visual feedback in the PyBullet simulation window

## ï¿½ Installation

### âœ… Prerequisites

- Python 3.9+
- pip

### ğŸ“¦ Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/Assistive-Arm-For-Cooking.git
   cd Assistive-Arm-For-Cooking
   ```

2. Create a virtual environment (optional but recommended):
   ```bash
   python -m venv cooking_arm
   source cooking_arm/bin/activate 
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt 
   ```

## ğŸ“· Gesture Instructions <br/>
To control the robotic arm, use the following finger gestures in front of your webcam. The system tracks your index and middle fingers to detect actions.

ğŸŒ€ Stir
- Hold your index and middle fingers close together

- Keep them at the same height

- The green line on screen should be short

- Hold for 1 second until progress fills

ğŸ”„ Flip
- Spread your index and middle fingers horizontally

- Keep them at the same height

- The green line should be long and horizontal

- Hold for 1 second until progress fills

ğŸ’§ Pour
- Spread your index and middle fingers vertically

- One finger should be higher

- The green line should be long and vertical

- Hold for 1 second until progress fills

## ğŸ§  How It Works
Gesture Recognition
- Uses MediaPipe to track hand landmarks in real-time

- Calculates distance and orientation between index and middle fingers

- Detects gesture based on direction and length of the vector between them

Motion Mapping
Each gesture is mapped to a specific motion:

- stir_motion() â€“ circular trajectory

- flip_motion() â€“ lift + tilt

- pour_motion() â€“ forward tilt with hold

Simulation
- The robot arm is rendered in a PyBullet GUI

- A compound object (pan + handle) is attached to the robot's end effector

- All motions are animated using inverse kinematics and smooth transitions

## ï¿½ Running the Project
   ```bash
   python gesture_arm_control.py
   ```
- A PyBullet window should open

- Allow camera access

- Show the appropriate gesture in front of your webcam

- The arm should respond after detecting and validating your gesture

## ğŸ“„ Model Notes
- model.urdf is the URDF description of the robot (KUKA iiwa used)

- Additional models like plane.urdf are loaded from PyBullet's pybullet_data

## ğŸ“š References
- MediaPipe Hands

- PyBullet Physics Engine

- OpenCV

## ğŸ’¡ Future Improvements
- Add support for more gestures (e.g., scoop, mix)

- Integrate speech input for voice commands

- Deploy on a physical robotic arm using ROS
<br/>
<h3 align="center">Feel free to fork, clone, and build upon it! Happy cooking ğŸ¤–ğŸ³</h3>


